{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eval-0",
   "metadata": {},
   "source": [
    "# PrimeurVision - Evaluation du modele YOLOv8\n",
    "\n",
    "Evaluation du meilleur modele issu de l'entrainement sur le jeu de test.\n",
    "\n",
    "**Metriques** : mAP@50, mAP@50-95, Precision, Recall, AP par classe\n",
    "\n",
    "**Classes** : carotte, aubergine, citron, pomme_de_terre, radis, tomate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-1",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-3",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-5",
   "metadata": {},
   "source": [
    "## 3. Chargement du modele et du dataset\n",
    "\n",
    "Le modele doit avoir ete sauvegarde sur Google Drive apres l'entrainement (`My Drive/PrimeurVision/models/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_PATH = '/content/drive/MyDrive/PrimeurVision/models/best_yolov8n_primeurvision.pt'\n",
    "DATASET_SRC = '/content/drive/MyDrive/PrimeurVision/dataset'\n",
    "MODELS_DIR = '/content/drive/MyDrive/PrimeurVision/models'\n",
    "WORK_DIR = '/content/dataset'\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# Copie en local pour accelerer\n",
    "if os.path.exists(WORK_DIR):\n",
    "    shutil.rmtree(WORK_DIR)\n",
    "shutil.copytree(DATASET_SRC, WORK_DIR)\n",
    "\n",
    "# Charger la config du dataset\n",
    "data_yaml_path = os.path.join(WORK_DIR, 'data.yaml')\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "# Mettre a jour les chemins locaux\n",
    "data_config['path'] = WORK_DIR\n",
    "data_config['train'] = 'images/train'\n",
    "data_config['val'] = 'images/val'\n",
    "data_config['test'] = 'images/test'\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "CLASS_NAMES = data_config['names']\n",
    "\n",
    "# Charger le modele\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(f\"Modele charge : {MODEL_PATH}\")\n",
    "print(f\"Classes : {CLASS_NAMES}\")\n",
    "\n",
    "n_test = len(os.listdir(os.path.join(WORK_DIR, 'images', 'test')))\n",
    "print(f\"Images de test : {n_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-7",
   "metadata": {},
   "source": [
    "## 4. Evaluation quantitative sur le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val(data=data_yaml_path, split='test')\n",
    "\n",
    "print(\"Resultats sur le test :\")\n",
    "print(f\"  mAP@50    : {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95 : {metrics.box.map:.4f}\")\n",
    "print(f\"  Precision : {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall    : {metrics.box.mr:.4f}\")\n",
    "\n",
    "print(\"\\nAP@50 par classe :\")\n",
    "for i, class_name in CLASS_NAMES.items():\n",
    "    ap50 = metrics.box.ap50[i] if i < len(metrics.box.ap50) else 0\n",
    "    print(f\"  {class_name:20s} : {ap50:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-9",
   "metadata": {},
   "source": [
    "## 5. Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La matrice de confusion est generee automatiquement par model.val()\n",
    "eval_dir = str(metrics.save_dir)\n",
    "confusion_img = os.path.join(eval_dir, 'confusion_matrix.png')\n",
    "\n",
    "if os.path.exists(confusion_img):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(Image.open(confusion_img))\n",
    "    plt.axis('off')\n",
    "    plt.title('Matrice de confusion (test)')\n",
    "    plt.show()\n",
    "\n",
    "# Si les courbes d'entrainement ont ete sauvegardees sur Drive\n",
    "curves_img = os.path.join(MODELS_DIR, 'results.png')\n",
    "if os.path.exists(curves_img):\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.imshow(Image.open(curves_img))\n",
    "    plt.axis('off')\n",
    "    plt.title('Courbes d\\'entrainement')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-11",
   "metadata": {},
   "source": [
    "## 6. Resultats qualitatifs - predictions sur le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(WORK_DIR, 'images', 'test', '*.jpg'))\n",
    "sample_test = random.sample(test_images, min(6, len(test_images)))\n",
    "\n",
    "n_cols = min(3, len(sample_test))\n",
    "n_rows = (len(sample_test) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows))\n",
    "if len(sample_test) == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for ax, img_path in zip(axes, sample_test):\n",
    "    result = model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)[0]\n",
    "    ax.imshow(result.plot()[:, :, ::-1])\n",
    "    ax.set_title(os.path.basename(img_path), fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "for i in range(len(sample_test), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Predictions sur le test', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
