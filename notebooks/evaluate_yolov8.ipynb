{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eval-0",
   "metadata": {},
   "source": [
    "# PrimeurVision — Évaluation du modèle YOLOv8\n",
    "\n",
    "Évaluation du meilleur modèle issu de l'entraînement sur le **jeu de test** (36 images, jamais vues pendant l'entraînement).\n",
    "\n",
    "**Métriques** : mAP@50, mAP@50-95, Précision, Recall, AP par classe\n",
    "\n",
    "**Classes** : carotte (0), aubergine (1), citron (2), pomme_de_terre (3), radis (4), tomate (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-1",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-3",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-5",
   "metadata": {},
   "source": [
    "## 3. Chargement du modèle et du dataset\n",
    "\n",
    "Le modèle doit avoir été sauvegardé sur Google Drive après l'entraînement (`My Drive/PrimeurVision/models/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_PATH  = '/content/drive/MyDrive/PrimeurVision/models/best_yolov8n_primeurvision.pt'\n",
    "DATASET_SRC = '/content/drive/MyDrive/PrimeurVision/dataset'\n",
    "MODELS_DIR  = '/content/drive/MyDrive/PrimeurVision/models'\n",
    "WORK_DIR    = '/content/dataset'\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# Copie en local\n",
    "if os.path.exists(WORK_DIR):\n",
    "    shutil.rmtree(WORK_DIR)\n",
    "shutil.copytree(DATASET_SRC, WORK_DIR)\n",
    "\n",
    "# Charger et mettre à jour la config\n",
    "data_yaml_path = os.path.join(WORK_DIR, 'data.yaml')\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "data_config['path']  = WORK_DIR\n",
    "data_config['train'] = 'images/train'\n",
    "data_config['val']   = 'images/val'\n",
    "data_config['test']  = 'images/test'\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "CLASS_NAMES = data_config['names']\n",
    "\n",
    "# Charger le modèle\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(f\"Modèle chargé : {MODEL_PATH}\")\n",
    "print(f\"Classes : {list(CLASS_NAMES.values())}\")\n",
    "print(f\"Images de test : {len(os.listdir(os.path.join(WORK_DIR, 'images', 'test')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-7",
   "metadata": {},
   "source": [
    "## 4. Évaluation quantitative sur le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val(data=data_yaml_path, split='test', conf=CONF_THRESHOLD)\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(\"  RÉSULTATS SUR LE JEU DE TEST\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"  mAP@50    : {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95 : {metrics.box.map:.4f}\")\n",
    "print(f\"  Précision : {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall    : {metrics.box.mr:.4f}\")\n",
    "print(\"-\" * 45)\n",
    "print(\"  AP@50 par classe :\")\n",
    "for i, name in CLASS_NAMES.items():\n",
    "    ap50 = metrics.box.ap50[i] if i < len(metrics.box.ap50) else 0\n",
    "    bar  = '█' * int(ap50 * 20)\n",
    "    print(f\"  {name:20s} : {ap50:.4f}  {bar}\")\n",
    "print(\"=\" * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-9",
   "metadata": {},
   "source": [
    "## 5. Matrice de confusion et courbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = str(metrics.save_dir)\n",
    "\n",
    "# Matrice de confusion normalisée\n",
    "for fname in ['confusion_matrix_normalized.png', 'confusion_matrix.png']:\n",
    "    confusion_img = os.path.join(eval_dir, fname)\n",
    "    if os.path.exists(confusion_img):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(Image.open(confusion_img))\n",
    "        plt.axis('off')\n",
    "        plt.title('Matrice de confusion — jeu de test')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "# Courbe Précision-Rappel\n",
    "pr_img = os.path.join(eval_dir, 'PR_curve.png')\n",
    "if os.path.exists(pr_img):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(Image.open(pr_img))\n",
    "    plt.axis('off')\n",
    "    plt.title('Courbe Précision-Rappel — jeu de test')\n",
    "    plt.show()\n",
    "\n",
    "# Courbes d'entraînement (depuis Drive)\n",
    "curves_img = os.path.join(MODELS_DIR, 'results.png')\n",
    "if os.path.exists(curves_img):\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.imshow(Image.open(curves_img))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Courbes d'entraînement (phase 2)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-11",
   "metadata": {},
   "source": [
    "## 6. Résultats qualitatifs — Prédictions réussies\n",
    "\n",
    "Exemples de détections correctes sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(WORK_DIR, 'images', 'test', '*.jpg'))\n",
    "sample_test = random.sample(test_images, min(6, len(test_images)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for ax, img_path in zip(axes.flatten(), sample_test):\n",
    "    result = model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)[0]\n",
    "    ax.imshow(result.plot()[:, :, ::-1])\n",
    "    n_det = len(result.boxes)\n",
    "    ax.set_title(\n",
    "        f\"{os.path.basename(img_path)[:25]}\\n({n_det} détection(s))\",\n",
    "        fontsize=8\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Exemples de prédictions réussies — jeu de test', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-13",
   "metadata": {},
   "source": [
    "## 7. Analyse des erreurs — Cas difficiles\n",
    "\n",
    "On identifie les images où le modèle détecte peu ou avec une faible confiance. Ces cas révèlent les limites du modèle : objets partiellement visibles, occlusions, angles atypiques, ou classes sous-représentées dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter toutes les prédictions sur le jeu de test\n",
    "all_results = []\n",
    "for img_path in test_images:\n",
    "    result   = model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)[0]\n",
    "    n_det    = len(result.boxes)\n",
    "    max_conf = float(result.boxes.conf.max()) if n_det > 0 else 0.0\n",
    "    all_results.append((img_path, result, n_det, max_conf))\n",
    "\n",
    "# Trier par confiance maximale croissante (les cas les plus difficiles en premier)\n",
    "all_results.sort(key=lambda x: x[3])\n",
    "\n",
    "# Afficher les 6 images les plus problématiques\n",
    "worst = all_results[:min(6, len(all_results))]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for ax, (img_path, result, n_det, max_conf) in zip(axes.flatten(), worst):\n",
    "    ax.imshow(result.plot()[:, :, ::-1])\n",
    "    ax.set_title(\n",
    "        f\"{os.path.basename(img_path)[:25]}\\n\"\n",
    "        f\"{n_det} det. | conf max = {max_conf:.2f}\",\n",
    "        fontsize=8, color='crimson'\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Cas difficiles — confiance maximale la plus faible', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résumé\n",
    "n_zero = sum(1 for _, _, n, _ in all_results if n == 0)\n",
    "confs  = [c for _, _, n, c in all_results if n > 0]\n",
    "print(f\"Images sans aucune détection : {n_zero}/{len(all_results)}\")\n",
    "if confs:\n",
    "    print(f\"Confiance moyenne (images avec détections) : {np.mean(confs):.3f}\")\n",
    "    print(f\"Confiance médiane : {np.median(confs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-15",
   "metadata": {},
   "source": [
    "## 8. Distribution des confiances et des classes détectées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = ['#FF8C00', '#9B59B6', '#FFD700', '#8B4513', '#E74C3C', '#FF4444']\n",
    "\n",
    "all_confs    = []\n",
    "class_counts = {v: 0 for v in CLASS_NAMES.values()}\n",
    "\n",
    "for img_path in test_images:\n",
    "    result = model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)[0]\n",
    "    for box in result.boxes:\n",
    "        all_confs.append(float(box.conf))\n",
    "        name = CLASS_NAMES.get(int(box.cls), f'class_{int(box.cls)}')\n",
    "        class_counts[name] = class_counts.get(name, 0) + 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des scores de confiance\n",
    "ax1.hist(all_confs, bins=20, color='steelblue', edgecolor='white')\n",
    "ax1.axvline(CONF_THRESHOLD, color='red', linestyle='--', label=f'Seuil = {CONF_THRESHOLD}')\n",
    "ax1.set_title('Distribution des scores de confiance (test)')\n",
    "ax1.set_xlabel('Confiance')\n",
    "ax1.set_ylabel('Nombre de détections')\n",
    "ax1.legend()\n",
    "\n",
    "# Nombre de détections par classe prédite\n",
    "bars = ax2.bar(class_counts.keys(), class_counts.values(), color=COLORS)\n",
    "ax2.bar_label(bars)\n",
    "ax2.set_title('Détections par classe (jeu de test)')\n",
    "ax2.set_xlabel('Classe')\n",
    "ax2.set_ylabel('Nb détections')\n",
    "ax2.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total détections : {len(all_confs)}\")\n",
    "if all_confs:\n",
    "    print(f\"Confiance moyenne : {np.mean(all_confs):.3f}\")\n",
    "    print(f\"Confiance médiane : {np.median(all_confs):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
