{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PrimeurVision — Fine-tuning YOLOv8\n",
    "\n",
    "Fine-tuning d'un modèle **YOLOv8n pré-entraîné sur COCO** pour détecter 6 classes de fruits et légumes en contexte culinaire.\n",
    "\n",
    "**Stratégie d'entraînement en 2 phases :**\n",
    "1. **Backbone gelé** : seule la tête de détection est entraînée — on adapte rapidement le modèle à nos classes sans perturber les features générales apprises sur COCO.\n",
    "2. **Fine-tuning complet** : on dégèle tout le réseau et on affine avec un learning rate plus faible — permet d'optimiser finement toutes les couches sur notre domaine.\n",
    "\n",
    "**Classes** : carotte (0), aubergine (1), citron (2), pomme_de_terre (3), radis (4), tomate (5)\n",
    "\n",
    "**Dataset** : 238 images — 166 train / 36 val / 36 test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5-md",
   "metadata": {},
   "source": [
    "## 3. Hyperparamètres\n",
    "\n",
    "| Paramètre | Phase 1 | Phase 2 | Justification |\n",
    "|---|---|---|---|\n",
    "| Epochs | 10 | 40 | Phase 1 courte : convergence rapide de la tête. Phase 2 plus longue : affinage fin |\n",
    "| Learning rate | 1e-2 | 1e-3 | LR élevé en phase 1 pour adapter vite la tête ; LR faible en phase 2 pour ne pas écraser les features |\n",
    "| Freeze | 10 couches | 0 | On gèle le backbone (couches 0–9) en phase 1, puis on libère tout |\n",
    "| Batch size | 16 | 16 | Compromis mémoire/stabilité des gradients sur Colab GPU |\n",
    "| Image size | 640 | 640 | Taille standard YOLOv8, bon compromis précision/vitesse |\n",
    "| Patience | 10 | 15 | Early stopping : arrêt si la mAP@50 ne s'améliore pas pendant N epochs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HYPERPARAMÈTRES ---\n",
    "\n",
    "# Phase 1 : backbone gelé\n",
    "PHASE1_EPOCHS  = 10\n",
    "PHASE1_LR      = 1e-2\n",
    "FREEZE_LAYERS  = 10\n",
    "\n",
    "# Phase 2 : fine-tuning complet\n",
    "PHASE2_EPOCHS  = 40\n",
    "PHASE2_LR      = 1e-3\n",
    "\n",
    "# Paramètres communs\n",
    "IMG_SIZE       = 640\n",
    "BATCH_SIZE     = 16\n",
    "PATIENCE_P1    = 10\n",
    "PATIENCE_P2    = 15\n",
    "CONF_THRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. Montage Google Drive et chargement du dataset\n",
    "\n",
    "Uploadez le dossier `dataset/` sur Google Drive dans `My Drive/PrimeurVision/dataset/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATASET_SRC = '/content/drive/MyDrive/PrimeurVision/dataset'\n",
    "WORK_DIR    = '/content/dataset'\n",
    "\n",
    "# Copie en local pour accélérer l'entraînement\n",
    "if os.path.exists(WORK_DIR):\n",
    "    shutil.rmtree(WORK_DIR)\n",
    "shutil.copytree(DATASET_SRC, WORK_DIR)\n",
    "\n",
    "# Charger et mettre à jour la config avec les chemins locaux\n",
    "data_yaml_path = os.path.join(WORK_DIR, 'data.yaml')\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "data_config['path']  = WORK_DIR\n",
    "data_config['train'] = 'images/train'\n",
    "data_config['val']   = 'images/val'\n",
    "data_config['test']  = 'images/test'\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "CLASS_NAMES = data_config['names']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"{NUM_CLASSES} classes : {list(CLASS_NAMES.values())}\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    img_dir = os.path.join(WORK_DIR, 'images', split)\n",
    "    if os.path.exists(img_dir):\n",
    "        print(f\"  {split}: {len(os.listdir(img_dir))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 5. Exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = ['#FF8C00', '#9B59B6', '#FFD700', '#8B4513', '#E74C3C', '#FF4444']\n",
    "\n",
    "def parse_yolo_label(label_path):\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "                annotations.append((class_id, x_center, y_center, width, height))\n",
    "    return annotations\n",
    "\n",
    "def plot_image_with_boxes(img_path, label_path, ax):\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    ax.imshow(img)\n",
    "    for class_id, xc, yc, bw, bh in parse_yolo_label(label_path):\n",
    "        x1 = (xc - bw / 2) * w\n",
    "        y1 = (yc - bh / 2) * h\n",
    "        color = COLORS[class_id % len(COLORS)]\n",
    "        rect = patches.Rectangle((x1, y1), bw * w, bh * h,\n",
    "                                  linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 5, CLASS_NAMES.get(class_id, '?'), color=color,\n",
    "                fontsize=9, fontweight='bold', backgroundcolor='black')\n",
    "    ax.axis('off')\n",
    "\n",
    "# 6 images aléatoires du train avec leurs bounding boxes\n",
    "train_images_dir = os.path.join(WORK_DIR, 'images', 'train')\n",
    "train_labels_dir = os.path.join(WORK_DIR, 'labels', 'train')\n",
    "image_files   = sorted(glob.glob(os.path.join(train_images_dir, '*.jpg')))\n",
    "sample_images = random.sample(image_files, min(6, len(image_files)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for ax, img_path in zip(axes.flatten(), sample_images):\n",
    "    base_name  = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_path = os.path.join(train_labels_dir, base_name + '.txt')\n",
    "    if os.path.exists(label_path):\n",
    "        plot_image_with_boxes(img_path, label_path, ax)\n",
    "plt.suptitle('Échantillon du dataset (train) avec annotations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des classes (nb images par classe par split)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, split in zip(axes, ['train', 'val', 'test']):\n",
    "    labels_dir   = os.path.join(WORK_DIR, 'labels', split)\n",
    "    class_counts = {v: 0 for v in CLASS_NAMES.values()}\n",
    "    for lf in glob.glob(os.path.join(labels_dir, '*.txt')):\n",
    "        seen = set()\n",
    "        for class_id, *_ in parse_yolo_label(lf):\n",
    "            name = CLASS_NAMES.get(class_id)\n",
    "            if name and name not in seen:\n",
    "                class_counts[name] += 1\n",
    "                seen.add(name)\n",
    "    bars = ax.bar(class_counts.keys(), class_counts.values(), color=COLORS)\n",
    "    ax.bar_label(bars)\n",
    "    ax.set_title(f'{split} ({sum(class_counts.values())} images)')\n",
    "    ax.set_ylim(0, max(class_counts.values()) + 5)\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "plt.suptitle('Distribution des classes par split', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Entraînement en 2 phases\n",
    "\n",
    "### Phase 1 — Backbone gelé\n",
    "\n",
    "On charge YOLOv8n pré-entraîné sur COCO et on gèle les 10 premières couches (backbone CSP-DarkNet). Seule la tête de détection apprend. Cette approche permet :\n",
    "- D'éviter le *catastrophic forgetting* des features génériques (bords, textures, formes)\n",
    "- D'obtenir une convergence rapide sur nos classes cibles\n",
    "- De réduire le risque d'overfitting avec notre petit dataset (~166 images train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "results_phase1 = model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=PHASE1_EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    lr0=PHASE1_LR,\n",
    "    freeze=FREEZE_LAYERS,\n",
    "    name='primeurvision_phase1',\n",
    "    patience=PATIENCE_P1,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19-md",
   "metadata": {},
   "source": [
    "### Phase 2 — Fine-tuning complet\n",
    "\n",
    "On reprend le meilleur modèle de la phase 1 et on dégèle toutes les couches. Le learning rate est réduit d'un facteur 10 pour affiner les poids sans écraser les représentations apprises lors de la phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_phase1 = os.path.join('runs', 'detect', 'primeurvision_phase1', 'weights', 'best.pt')\n",
    "model = YOLO(best_phase1)\n",
    "\n",
    "results_phase2 = model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=PHASE2_EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    lr0=PHASE2_LR,\n",
    "    freeze=0,\n",
    "    name='primeurvision_phase2',\n",
    "    patience=PATIENCE_P2,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "RESULTS_DIR = os.path.join('runs', 'detect', 'primeurvision_phase2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Courbes d'entraînement et résultats sur la validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes loss + mAP au fil des epochs (phase 2)\n",
    "results_img = os.path.join(RESULTS_DIR, 'results.png')\n",
    "if os.path.exists(results_img):\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.imshow(Image.open(results_img))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Courbes d'entraînement — Phase 2 (fine-tuning complet)\")\n",
    "    plt.show()\n",
    "\n",
    "# Matrice de confusion normalisée (validation)\n",
    "for fname in ['confusion_matrix_normalized.png', 'confusion_matrix.png']:\n",
    "    confusion_img = os.path.join(RESULTS_DIR, fname)\n",
    "    if os.path.exists(confusion_img):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(Image.open(confusion_img))\n",
    "        plt.axis('off')\n",
    "        plt.title('Matrice de confusion — validation (phase 2)')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "# Courbe Précision-Rappel\n",
    "pr_img = os.path.join(RESULTS_DIR, 'PR_curve.png')\n",
    "if os.path.exists(pr_img):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(Image.open(pr_img))\n",
    "    plt.axis('off')\n",
    "    plt.title('Courbe Précision-Rappel par classe')\n",
    "    plt.show()\n",
    "\n",
    "# Métriques finales sur la validation\n",
    "metrics_val = model.val(data=data_yaml_path, split='val')\n",
    "print(\"\\n=== Résultats sur la validation (phase 2) ===\")\n",
    "print(f\"  mAP@50    : {metrics_val.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95 : {metrics_val.box.map:.4f}\")\n",
    "print(f\"  Précision : {metrics_val.box.mp:.4f}\")\n",
    "print(f\"  Recall    : {metrics_val.box.mr:.4f}\")\n",
    "print(\"\\nAP@50 par classe :\")\n",
    "for i, name in CLASS_NAMES.items():\n",
    "    ap = metrics_val.box.ap50[i] if i < len(metrics_val.box.ap50) else 0\n",
    "    print(f\"  {name:20s} : {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde du modèle sur Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/content/drive/MyDrive/PrimeurVision/models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Meilleur modèle (phase 2)\n",
    "best_model_path = os.path.join(RESULTS_DIR, 'weights', 'best.pt')\n",
    "shutil.copy2(best_model_path, os.path.join(save_dir, 'best_yolov8n_primeurvision.pt'))\n",
    "\n",
    "# Courbes, matrices et figures utiles pour le rapport\n",
    "for fname in ['results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png',\n",
    "              'PR_curve.png', 'F1_curve.png']:\n",
    "    src = os.path.join(RESULTS_DIR, fname)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, os.path.join(save_dir, fname))\n",
    "\n",
    "print(f\"Modèle et courbes sauvegardés dans : {save_dir}\")\n",
    "print(f\"  → best_yolov8n_primeurvision.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
